---
title: "Predicción de Abandono"
author: "Tarea 2, Grupo 5"
date: "2021"
output: pdf_document
urlcolor: blue
graphics: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE,
                      fig.width = 7, fig.height = 4, fig.align = "right")
```

```{r, warning=FALSE}
library(tidyverse)
library(data.table)
library(broom)
library(knitr)
library(lubridate)
library(RCT)
library(gamlr)
library(ranger)
library(tree)
library(parallel)
library(tidymodels)
library(randomForest)
library(fastDummies)
library(dplyr)
library(parallel)
library(ROCR)
library(yardstick)
library(ggplot2)
library("gridExtra")
```



## Contexto

Cell2Cell es una compañía de teléfonos celulares que intenta mitigar el abandono de sus usuarios. Te contratan para 1) Encontrar un modelo que prediga el abandono con acierto y para usar los insights de este modelo para proponer una estrategia de manejo de abandono.


Las preguntas que contestaremos son:

1. Se puede predecir el abandono con los datos que nos compartieron? 

Si se puede, tenemos datos que nos ayudan a ver que usuarios se quedaron y cuales abandonaron (churn). En promedio 29% de los clientes han abandonado entre 30 y 60 días después de su fecha de observación y el resto siguen siendo clientes. 

2. Cuáles son las variables que explican en mayor medida el abandono? 

3. Qué incentivos da Cell2Cell a sus usarios para prevenir el abandono?

4. Cuál es el valor de una estrategia de prevención de abandono focalizada y cómo difiere entre los segmentos de los usuarios? Qué usuarios deberían de recibir incentivos de prevención? Qué montos de incentivos

Nota: Voy a evaluar las tareas con base en la respuesta a cada pregunta. Como hay algunas preguntas que no tienen una respuesta clara, al final ponderaré de acuerdo al poder predictivo de su modelo vs las respuestas sugeridas. 


\newpage

## Datos

Los datos los pueden encontrar en `Cell2Cell.Rdata`. En el archivo `Cell2Cell-Database-Documentation.xlsx` pueden encontrar documentación de la base de datos. 

Cargemos los datos
```{r cargamos los datos}
setwd('C:/Users/Jose Pablo Garcia/Box Sync/A - JP Files/Personal/ITAM/Economía Computacional/Tarea 2')
load('Cell2Cell.Rdata')
```

### 1. Qué variables tienen missing values? Toma alguna decisión con los missing values. Justifica tu respuesta

Hay muy pocos missing values, vamos a eliminarlos todos.

```{r 1.Missing values}
faltantes <- map_dbl(cell2cell%>%select_all, function(x) 100*sum(is.na(x))/nrow(cell2cell))
faltantes <- data.frame(faltantes[faltantes>0])

colnames(faltantes) <- "Porcentaje Faltantes"

faltantes <- faltantes %>% 
  arrange(desc(`Porcentaje Faltantes`))

kable(faltantes, caption = "Tabla para variables NA",
      col.names = c("Porcentaje"), 
      digits = 3)

```

Hay pocas observaciones faltantes, distribuidas en las siguientes columnas:
  recchrge - recurring charge, el valor promedio del plan de consumo;
  overage - el exceso de minutos de llamadas;
  roam - el número de llamadas en roaming;
  changem - el cambio % en la utilización de minutos;
  changer - el cambio % en el gasto; 
  revenue - el gasto promedio; 
  mou - los minutos utilizados por mes;
  phones - el número de teléfonos que el usario ha utilizado;
  models - el número de modelos de teléfono que el usuario ha utilizado;
  age1 - la edad del 1er miembro del hogar;
  age2 - la edad del 2do miembro del hogar;
  eqpdays - el número de dias com el teléfono; y
  directas - el número promedio de llamadas con asistencia.
Hay observaciones que no podemos estimar o soponer y, así, se propone quitarlas
de la base. Son las observaciones contenidas en: recchrge, overage, changem, 
changer, eqpdays, revenue y mou.
Otras observaciones podemos suponer que los NAs serían 0 (roam y directas) o que 
tienen el valor de 1 (phones y models. O sea, se supone que el usurario ha 
tenido apenas un teléfono con la comopañia).
La variable de edad (age1 y age2) tiene pocos NAs, pero presenta vários valores
iguales a 0 (más de 20 mil observaciones). Así, se propone sustituyer los NAs y
los valores 0 por el promedio.
Además de NAs, la variable de días con el equipo (eqpdays) y de costo del plan 
(rechchrege) presentan unos pocos valores negativos que no hacen sentido. 
Se propone quitar los valores negativos de la base. 



```{r quitamos NA y reemplazamos datos}

# Quitando de la base
cell2cell <- cell2cell %>%
  filter(!is.na(changem),
         !is.na(changer),
         !is.na(revenue),
         !is.na(mou),
         !is.na(recchrge),
         !is.na(overage),
         !is.na(eqpdays))

# Igualando a 0 o 1
cell2cell$roam[cell2cell$roam == "NA"] <- 0
cell2cell$directas[cell2cell$directas == "NA"] <- 0
cell2cell$phones[cell2cell$phones == "NA"] <- 1
cell2cell$models[cell2cell$models == "NA"] <- 1

# Igualando al promedio
cell2cell$age1[cell2cell$age1 == 0] <- mean(cell2cell$age1)
cell2cell$age1[cell2cell$age2 == 0] <- mean(cell2cell$age2)
cell2cell$age1[is.na(cell2cell$age1)] <- mean(cell2cell$age1, na.rm = T)
cell2cell$age2[is.na(cell2cell$age2)] <- mean(cell2cell$age2, na.rm = T)

# Quitando valores negativos de la base
cell2cell <- cell2cell[cell2cell$eqpdays > 0 & cell2cell$recchrge >0]

# Verificando que no quedan más NAs
faltantes <- map_dbl(cell2cell%>%select_all, function(x) 100*sum(is.na(x))/nrow(cell2cell))
faltantes <- data.frame(faltantes[faltantes>0])

colnames(faltantes) <- "Porcentaje Faltantes"

faltantes <- faltantes %>% 
  arrange(desc(`Porcentaje Faltantes`))
```

### 2. Tabula la distribución de la variable `churn`. Muestra la frecuencia absoluta y relativa. Crees que se debe hacer oversampling/undersamping?  

Conviene hacer undersampling, ya que los datos del estudio de interés son la mitad de los que tenemos en el otro grupo.
Por lo tanto creo será mejor así. Además evitamos generar variables sintéticas.

```{r 2.Distr 'churn'}
#Obtengo los datos para hacer mi tabla de frecuencias -
valores <- c(0,1)
freq_abs <- summary(as.factor(cell2cell$churn))
freq.tab <- as.data.frame(cbind(valores, freq_abs))
freq.tab <- freq.tab %>%
  mutate(freq_rel = freq_abs/sum(freq_abs))
knitr::kable(freq.tab,
             caption = "Tabla Abandono: frecuencia relativa y absoluta",
             digits = 2)
```

### 3. (2 pts) Divide tu base en entrenamiento y validación (80/20). Además, considera hacer oversampling (SMOTE) o undersampling. (Tip: Recuerda que el objetivo final es tener muestra ~balanceada en el traning set. En el validation la distribución debe ser la original)

Vamos a hacer el Undersampling. Para esto sólo necesitamos utilizar la librería sample incluida en R.
Esto nos permite tomar muestras aleatorias, unicamente hay que decirle de que tamaño, en este caso serán del grupo más chico (los que se abandonaron o churn=1).

```{r 3.Split sample}
# Vamos a hacer una distribucion 80 - 20 entre el training y validation set
# Generamos una variable de cuartiles de revenue para considerar como estrato en la division de la base
cell2cell <- cell2cell%>%
  mutate(cuartiles_revenue = ntile_label(revenue,4,0))
#Hacemos la aleatorizacion para dividir la base
asignacion <- treatment_assign(cell2cell, 
                               share_control = 0.80,
                               n_t = 1,
                               strata_varlist = c("churn","cuartiles_revenue"),
                               seed = 1994,
                               key="customer")
list2env(asignacion, envir = .GlobalEnv)
cell2cell <- left_join(cell2cell,
                       data%>%ungroup()%>%select(customer,treat),
                       by="customer")
cell2cell$cuartiles_revenue <- NULL
```

Dividimos la base en entrenamiento y validacion y revisamos que la base de entrenamiento se encuentra balanceada por clases
```{r}
# Base de entrenamiento (80%) 
training_set <- cell2cell%>%
  filter(treat==0)
#Base de validacion (20%)
validation_set <- cell2cell%>%
  filter(treat==1)
rm(asignacion, data, summary_strata, variables)
# Realizamos un undersampling para reducir las observaciones de la clase mas comun. Esto mediante un muestreo
# estratificado. Las ventajas de este metodo sobre el oversampling es que no generas info sintentica.
asignacion <- treatment_assign(training_set%>%filter(churn==0),
                               share_control = 0.4055,
                               n_t = 1,
                               strata_varlist = c("churn"),
                               seed = 1994,
                               key="customer")
list2env(asignacion, envir = .GlobalEnv)
## Juntamos la asignacion con el training set
training_set <- left_join(training_set%>%select(-treat),
                         data%>%ungroup()%>%select(customer,treat),
                         by="customer")
training_set <- training_set%>%
  filter(treat==0 | is.na(treat))%>%
  select(-treat)
rm(asignacion, data, summary_strata, variables)
prop.table(table(training_set$churn))
table(training_set$churn)
```


## Model estimation

Pondremos a competir 3 modelos: 

1. Cross-Validated LASSO-logit

2. Prune Trees

3. Random Forest

### 4 (2 pts). Estima un cross validated LASSO. Muestra el la gráfica de CV Binomial Deviance vs Complejidad

Vamos a realizar el CV LASSO-logit para el set de entrenamiento partido normal y el partido undersampling.
Eso nos permitirá ver la diferencia en los estimadores si no consideramos esto.

```{r 4. CV LASSO}
Xs <- training_set %>%select(-c(customer,churn))
Xs <- sparse.model.matrix(~.+0, data=Xs)
# La sparse model matrix en lugar de guardar todos los registros 0 y 1, solo guarda las posiciones donde hay 1s
# Entonces resume mucha info en una matriz mucho mas pequena. Podemos declara la formula que nosotros queremos
# Sin intercepto: .+0
# Interacciones con Variable: .+.*Variable 
# Polinomios .^2
y <- training_set$churn
#Cluster for parallelization
detectCores()
cl <- makeCluster(8) 
cl
# Estimar CV Logit Lasso on 5 folds
lasso_logit <- cv.gamlr(x=Xs, y=y, nfold=5, verb=T, cl = cl, family='binomial')

data = cell2cell
set.seed(101)
sample <- sample.int(n = nrow(data), size = floor(.80*nrow(data)), replace = F)
test  <- data[-sample, ]

# Prediciendo
y_pred<- predict(lasso$gamlr, test ,type ='response',select = lasso_logit$seg.min)
y_pred<-as.numeric(y_pred)

# Cuando corremos algo de forma paralelizado la computadora se hace menos poderosa
# entonces mejor volver a juntar los nucleos
stopCluster(cl)
save(lasso_logit, file = 'C:/Users/Jose Pablo Garcia/Box Sync/A - JP Files/Personal/ITAM/Economía Computacional/Tarea 2/models/lasso_logit.Rdata')
rm(cl, Xs)
```

### 5. Grafica el Lasso de los coeficientes vs la complejidad del modelo.   

```{r 5.LASSO coef-complex}
plot(lasso_logit)
plot(lasso_logit$gamlr)
```

\newpage

### 6 (2 pts). Cuál es la $\lambda$ resultante? Genera una tabla con los coeficientes que selecciona el CV LASSO. Cuántas variables deja iguales a cero? Cuales son las 3 variables más importantes para predecir el abandono? Da una explicación intuitiva a la última pregunta

Las 3 variables, sin contar el intercepto, con mayor importancia son: retcall, creditaa, refurb.
- retcall: llamadas realizadas por el cliente al equipo de retención
- creditaa: riesgo de crédito aa \{1=si\}
- refurb: si el auricular está reacondicionado para su venta, es decir equipos devueltos al fabricante, reparados y revendidos.

A priori, podríamos pensar que los clientes que marcaron al equipo de retencion buscan alguna promoción para no irse y al no conseguiirla abandonan, adicionalmente aquellos que tienen un mayor riesgo crediticio aa disminuye la probabilidad de irse. Por último aquellos que tienen equipos revendidos pueden ser más suceptibles a abandonar debido a que sus equipos son más baratos y por lo tanto de menor calidad, provocando que su servicio sea malo y por último se vayan antes.

```{r 6.Coefs CV LASSO}
lambda_min_under <- (lasso_logit$lambda.min)
lambda_1std_under <-(lasso_logit$lambda.1se)

tabla_lambda <- rbind(lambda_min_under, lambda_1std_under)

kable(tabla_lambda, caption = "Lambdas del modelo (under = undersample)",
      col.names = c("Lambda"), 
      digits = 8)

#Lambda resultante
cat("La Lambda que minimiza el modelo es: ",lambda_min_under,"\n")
cat("La Lambda que 1 Std del modelo es: ",lambda_1std_under,"\n")

#Tabla coefs que selecciona el LASSO
#coef(lasso, select="min")
B <- coef(lasso_logit)[-1,] 
B[c(which.min(B),which.max(B))]
coefs <- as.data.frame(B) %>%
  filter(B !=0)
kable(coefs,
      caption = "Coefs que selecciona el LASSO")
#Variables que deja en 0
coefs_0 <- as.data.frame(B) %>%
  filter(B == 0)
kable(coefs_0,
      caption = "Coefs que deja en 0 el LASSO")
#No. variables que deja en 0
nrow(coefs_0)
# 3 variables mas importantes
coefs <- abs(coefs) %>% arrange(desc(B))
coefs <- tibble::rownames_to_column(coefs, "Variable")

coefs_mas <- head(coefs, 4)
kable(coefs_mas, caption = "Coeficientes con mayor peso",
      col.names = c("Coeficiente", "|Coeficiente|"),
      ddigits = 3)
```

La variable 'retcall' se refiere al número de llamadas realizadas al usuario por el equipo de retención, en el que convencen al cliente de renovar y le ofrecen ofertas, por lo que tiene mucho sentido que sea una de las variablas más importantes para explicar la decisión de abandono o no.

Por otro lado, la variable 'creditaa' se refiere a una alta calificación crediticia, por lo que podría el cliente seguir utilizando su línea de crédito para seguir teniendo el servicio aún si no cuenta con la liquidez necesaria en algún momento.

Por último, la variable 'refurb' se refiere a la renovación del equipo, por lo que es lógico que ante dicha renovación cuando se acaba el mes de servicio, le inviten al cliente a continuar con el mismo y  no abandone.

### 7. Genera un data frame (usando el validation set) que tenga: `customer`, `churn` y las predicciones del LASSO. 

```{r 7.LASSO predic}
#########################################################################   MONTSE
library(dplyr)
eval <- validation_set %>%
                  select(customer, churn) %>%
                  mutate(churn_pred = y_pred)
eval <- eval %>%
  mutate(churn = as.factor(churn),
         y_pred = as.numeric(y_pred))
# Roc curve
roc <- yardstick::roc_curve(data=eval, truth = churn, y_pred)
ggplot(roc,aes(x = specificity, y = sensitivity))+geom_abline(slope = -1, intercept = 1, linetype ='dashed')+geom_path()+theme_bw()
roc_auc(eval, truth = churn, y_pred)
##########################################################################

#Prediccion en Base de Validacion
# Prediction Vectors

test_under<- select(test_under, -customer, -churn)
test_under <- as.numeric(unlist(test_under))
typeof(test_under)

X_validation <- sparse.model.matrix(~.+0, data=test_under)

rediccion_1 <- drop(predict(lasso_under$gamlr, X_validation,
                      type='response',
                      select = lasso_under$seg.min))


prediccion_2 <- drop(predict(lasso_under$gamlr, X_validation,
                      type='response',
                      select = lasso_under$seg.1se))

#response es el score de probabilidad, la prediccion del Lasso
#Que escoja el segmento minimo (donde se minimiza el error de pred fuera de la muesta)

eval_1 <- bind_cols(test_y_under, prediccion_1)
colnames(eval_1) <- cbind("obs","pred_1")
eval_1$obs <- as.factor(eval_1$obs)

eval_2 <- bind_cols(test_y_under, prediccion_2)
colnames(eval_2) <- cbind("obs","pred_2")
eval_2$obs <- as.factor(eval_2$obs)

curva_1 <- yardstick::roc_curve(data = eval_1, truth = obs, "pred_1")
curva_2 <- yardstick::roc_curve(data = eval_2, truth = obs, "pred_2")

#truth es la columna donde esta la y observada, la otra entrada es para epecificar cual es la clase positiva.

#Lo que te sale de esta funcion no es una curva, si no una tabla con la especificidad y sensibilidad resultantes para cada corte de probabilidad

# Para graficar la curva ROC
curva_roc_1 <- ggplot(curva_1, aes(x=specificity, y=sensitivity))+
  geom_abline(slope = -1, intercept = 1, linetype= 'dashed')+
  geom_path()+ # Para graficar el continuio de puntos
  theme_bw()


curva_roc_2 <- ggplot(curva_2, aes(x=specificity, y=sensitivity))+
  geom_abline(slope = -1, intercept = 1, linetype= 'dashed')+
  geom_path()+ # Para graficar el continuio de puntos
  theme_bw()

curva_roc_1
curva_roc_2

#ggsave(height = 6, width = 6, filename = "Graficas/roc_lasso.png")

# Para calcular el area debajo de la curva.
AUC_1 <- yardstick::roc_auc(eval_1, truth = obs, "pred_1")
AUC_2 <- yardstick::roc_auc(eval_2, truth = obs, "pred_2")

AUC_1
AUC_2

cat("El AUC del modelo de lambda minima es: ",AUC_1$.estimate,"\n",
    "El AUC del modelo de lambda 1 Std es: ",AUC_2$.estimate,"\n",
    "Por lo tanto: un chimpance aventando dardos lo hubiera hecho mejor")
```


### 8. Estima ahora tree. Usa `mindev = 0.05, mincut = 1000` Cuántos nodos terminales salen? Muestra el summary del árbol

```{r 8.Tree}
# Separo mi muestra en train y test sets.
set.seed(100)
train <- sample(1:nrow(cell2cell), size = nrow(cell2cell)*4/5)
test <- cell2cell[-train,]
# Tree
arbol_clasificacion <- tree(formula = churn ~ ., data = cell2cell, subset = train,
                            control = tree.control(nobs= length(train), 
                                                   mincut = 1000,  mindev = 0.05))
save(arbol_clasificacion, file = "C:/Users/Jose Pablo Garcia/Box Sync/A - JP Files/Personal/ITAM/Economía Computacional/Tarea 2/models/arbol.Rdata")
summary(arbol_clasificacion)
```


### 9. Grafica el árbol resultante 

No se puede graficar, ya que solo tiene un nodo.

```{r 9. Tree chart}
plot(arbol_clasificacion); text(arbol_clasificacion, pretty = 0)
plot(arbol_clasificacion$weights)
```


### 10. Poda el árbol usando CV. Muestra el resultado. Grafica Tree Size vs Binomial Deviance. Cuál es el mejor tamaño del árbol? Mejora el Error?


```{r 10. CV tree}
data = cell2cell
set.seed(101) # Set Seed so that same sample can be reproduced in future also
sample <- sample.int(n = nrow(data), size = floor(.80*nrow(data)), replace = F)
train <- data[sample, ]
test  <- data[-sample, ]
validation = test
#--------------------------------------------------------------------------------
# Undersampling in training set 
library(ROSE)
# imbalance on training set
table(train$churn)
# balanced data set with under-sampling
train_un <- ovun.sample(churn~., data=train,
                                  p=0.5, seed=1, 
                                  method="under")$data


train_un <- train_un %>% select(-customer)
#Vulevo a hacer el árbol normal sin tomar mindev y mincut para que me salga >1 nodo
arbol2 <- tree(churn ~. , data = train_un)
set.seed(3)
cv_arbol <- cv.tree(arbol2, FUN = prune.tree, K = 5)
plot(cv_arbol$size, cv_arbol$dev , type = "b",
     xlab = "Tree Size", ylab = "CV Misclassification Rate")
# No. nodos óptimos
paste("Un árbol con:", cv_arbol$size[which.min(cv_arbol$dev)], "nodos terminales minimiza el test error.")

```

Un árbol con 2 nodos terminales minimiza el test error.


### 11. Gráfica el árbol final. (Tip: Checa `prune.tree`)

```{r 11. Pruned tree}
arbol_pruning <- prune.tree(tree = arbol2, best = 2)
plot(x = arbol_pruning, type = "proportional")
text(x = arbol_pruning, splits = TRUE, pretty = 0,
     cex = 0.8, col = "purple")
title(main = "Pruned Tree")
```



### 12. Genera las predicciones del árbol pruned. Guardalas en la base de predicciones. Guarda el score y la prediccion categorica en la misma data frame donde guardaste las predicciones del LASSO

```{r 12. Predic pruned tree}
#MON
# Prediccion 
y_pred_pruned <- predict(arbol_pruning, newdata = testx,
                        type = "vector")
# Matriz de confusión
table(predicted = y_pred_pruned, actual = test$churn)
#Guardamos prediccion del arbol junto a la del LASSO
predicciones <- eval %>% select(customer, churn, y_pred) %>%
                         rename(Y_lasso = y_pred) %>%
                         mutate(Y_prunedtree = as.numeric(y_pred_pruned))
```



### 13 (4pts). Corre un Random Forest ahora. Cuál es la $B$ para la que ya no ganamos mucho más en poder predictivo?

- Corre para `num.trees=100,200,300, 500, 700, 800`

- En cada caso, guarda únicamente el `prediction.error`

```{r 13. Random forest}
detectCores()
cl<-makeCluster(8)
cl
# Estimation
errores <- map_dbl(c(100,200,300,500,700,800),
                   function(x) ranger(churn~., data = training_set%>%select(-customer),
                      classification = T,
                      num.trees = x,
                      importance = "impurity",
                      verbose = T)$prediction.error)
errores <- tibble( Num_trees = c(100,200,300,500,700,800),
                   pred_error = errores)
ggplot(errores, aes(x=Num_trees, y=pred_error))+
  geom_path()+ 
  theme_bw()+
  labs(y = "Test Classification Error",x = "Number of Trees")+
  theme(axis.title.y = element_text(size = 12),axis.title.x = element_text(size = 12),
        axis.text.y=element_text(size=11),axis.text.x=element_text(size=11),
        legend.text = element_text(size = 11))+ 
  scale_x_continuous(labels = comma,limits = c(100,800),breaks=seq(100,800,100))+
  ggsave(height = 6, width = 6, filename = "Graficas/rf_number_trees.png")


#MONTSE BACKUP
detectCores()
cl<-makeCluster(8)
cl
# Estimation
a<-Sys.time()
rf_100 <-ranger(churn~., data = train_un, classification = T, num.trees = 100)
rf_200 <-ranger(churn~., data = train_un, classification = T, num.trees = 200)
rf_300 <-ranger(churn~., data = train_un, classification = T, num.trees = 300)
rf_500 <-ranger(churn~., data = train_un, classification = T, num.trees = 500)
rf_700 <-ranger(churn~., data = train_un, classification = T, num.trees = 700)
rf_800 <-ranger(churn~., data = train_un, classification = T, num.trees = 800, 
                importance = 'impurity_corrected')
Sys.time()-a
#save(rf, file ='rf_100.Rdata')
stopCluster(cl)
# Guardar prediction error del OOB
MSE_100 <- rf_100$prediction.error
MSE_200 <- rf_200$prediction.error
MSE_300 <- rf_300$prediction.error
MSE_500 <- rf_500$prediction.error
MSE_700 <- rf_700$prediction.error
MSE_800 <- rf_800$prediction.error
 
MSE <- as.data.frame(cbind(rf_100$prediction.error, rf_200$prediction.error, rf_300$prediction.error,
               rf_500$prediction.error, rf_700$prediction.error, rf_800$prediction.error ))
No.Tree <- cbind(100,200,300,500,700,800)
MSE_RF <- t(as.data.frame(rbind(No.Tree, MSE)))
kable(MSE_RF,
      col.names = c("No.Tree", "MSE"), 
      caption = "Prediction error" ) 

```



### 14. Escoge un random forest para hacer las predicciones. Grafica la importancia de las variables. Interpreta 

```{r 14. Predic RF e importancia var}
t0<-Sys.time()
random_forest <- ranger(churn~., 
                        data = training_set%>%select(-customer),
                        classification = T,
                        num.trees = 500,
                        importance = "impurity",
                        verbose = T)
Sys.time() -t0
save(random_forest, file = 'C:/Users/Jose Pablo Garcia/Box Sync/A - JP Files/Personal/ITAM/Economía Computacional/Tarea 2/models/random_forest.Rdata')
stopCluster(cl)
# Generamos un data frame con la importancia de las variables
var_importance <- data.frame(importance(random_forest))
var_importance <- bind_cols(variable = rownames(var_importance),var_importance)
rownames(var_importance) <- 1:nrow(var_importance)

#MONTSE
important_var <- as.data.frame(importance_pvalues(rf_800)) %>%
  filter(pvalue < 0.01) %>%
  arrange(desc(importance))
kable(important_var,
      caption = "Most important variables (pvalue<0.01)")

```


### 15. Genera las predicciones OOS para el random forest. Guardalas en la misma data.frame que los otros modelos 

```{r 15. Predic OOS}
# Generamos las predicciones puntuales del modelo


#MONTSE
#Por efectos prácticos selecciono el RF con 700 árboles
# Out of Bag:  
oob <- rf_800$predictions
# Prediction vectors:  
pred_val<-predict(rf_800, data = validation)$predictions
#Guardamos prediccion del random forest junto al resto 
predicciones <- predicciones %>% 
                         mutate(Y_randomfor = as.numeric(pred_val))




#FAJARDO
pred_800_p2 <- predict(rf_800_p2, data = test_under, type = 'response')
predicciones$rf <- pred_800_p2$predictions


```



### 16 (2pts). Corre el mismo forest pero ahora con `probability = T`. Esto generará predicciones númericas en lugar de categóricas. Genera las predicciones continuas y guardalas en el mismo data frame

```{r 16. Predict RF numérico}
#CARLOS
rf_700_cont <-ranger(churn~., data = train_un, probability = T, num.trees = 700)
# Prediction vectors:  
pred_val1<-predict(rf_700_cont, data = validation)$predictions
#Guardamos prediccion del random forest junto al resto 
predicciones <- predicciones %>% 
                         mutate(Y_randomfor_prob0 = as.numeric(pred_val1[,1]),
                                Y_randomfor_prob1 = as.numeric(pred_val1[,2]))

```



### 17 (4 pts). Genera graficas de las curvas ROC para los tres modelos. Cual parece ser mejor?

```{r 17. Curvas ROC}
#CARLOS
roc_lasso <- roc_curve(data = evaluacion%>%select(churn,pred_prob_lasso)%>%rename(pred=pred_prob_lasso),
                       truth = churn, pred) 
roc_tree <- roc_curve(data = evaluacion%>%select(churn,pred_prob_tree)%>%rename(pred=pred_prob_tree), 
                      truth = churn, pred) 
roc_rf <- roc_curve(data = evaluacion%>%select(churn, pred_prob_forest)%>%rename(pred=pred_prob_forest),
                    truth = churn, pred) 
# Para graficar la curva ROC del LASSO
ggplot(roc_lasso, aes(x=specificity, y=sensitivity))+
  geom_abline(slope = -1, intercept = 1, linetype= 'dashed')+
  geom_path()+ 
  theme_bw()
# Grafica curva ROC del arbol podado
ggplot(roc_tree, aes(x=specificity, y=sensitivity))+
  geom_abline(slope = -1, intercept = 1, linetype= 'dashed')+
  geom_path()+ 
  theme_bw()
# Grafica curva ROC del arbol podado
ggplot(roc_rf, aes(x=specificity, y=sensitivity))+
  geom_abline(slope = -1, intercept = 1, linetype= 'dashed')+
  geom_path()+ 
  theme_bw()

#MON
#Curvas ROC para LASSO, Tree y Random Forest 
roc_lasso <- roc_curve(data=predicciones, truth = churn, Y_lasso)
roc_pruned <- roc_curve(data=predicciones, truth = churn, Y_prunedtree)
roc_rf <- roc_curve(data=predicciones, truth = churn, Y_randomfor_prob1)
g1 <- ggplot(roc_lasso,aes(x = specificity, y = sensitivity))+geom_abline(slope = -1, intercept = 1, linetype ='dashed')+geom_path(col = "plum3")+theme_bw()+
    labs(title = "CV LASSO") +
    theme(plot.title = element_text(hjust = 0.5))
g2 <- ggplot(roc_pruned,aes(x = specificity, y = sensitivity))+geom_abline(slope = -1, intercept = 1, linetype ='dashed')+geom_path(col = "pink")+theme_bw()+
    labs(title = "CV Pruned tree") +
    theme(plot.title = element_text(hjust = 0.5))
g3 <- ggplot(roc_rf,aes(x = specificity, y = sensitivity))+geom_abline(slope = -1, intercept = 1, linetype ='dashed')+geom_path(col = "lightblue3")+theme_bw()+
    labs(title = "Random Forest") +
    theme(plot.title = element_text(hjust = 0.5))
grid.arrange(g1, g2, g3,
             ncol = 2)


#FAJARDO

predicciones$obs <- as.factor(predicciones$obs)
predicciones$pruned_score <- predicciones$pruned_score[,1]
predicciones$rf_prob <- predicciones$rf_prob[,1]

roc_lasso <- roc_curve(data = predicciones, truth = obs, "lasso_cvmin")

roc_tree <- roc_curve(data = predicciones, truth = obs, "pruned_score")

roc_rf <- roc_curve(data = predicciones, truth = obs, "rf_prob")

# Para graficar la curva ROC del LASSO

gg_roclasso <- ggplot(roc_lasso, aes(x=specificity, y=sensitivity))+
  geom_abline(slope = -1, intercept = 1, linetype= 'dashed')+
  geom_path()+ 
  labs(title = "Curva Roc",
       subtitle = "Modelo Logit Lasso CV",
       x = "Especificidad", y = "Sensibilidad")

gg_roclasso + theme_economist()+
  scale_color_economist()

ggsave(gg_roclasso, filename = "C:/Users/Jose Pablo Garcia/Box Sync/A - JP Files/Personal/ITAM/Economía Computacional/Tarea 2/graphs/roc_lasso.jpeg")

gg_roctree <- ggplot(roc_tree, aes(x=specificity, y=sensitivity))+
  geom_abline(slope = -1, intercept = 1, linetype= 'dashed')+
  geom_path()+ 
  labs(title = "Curva Roc",
       subtitle = "Modelo pruned tree",
       x = "Especificidad", y = "Sensibilidad")

gg_roctree + theme_economist()+
  scale_color_economist()

ggsave(gg_roctree, filename = "C:/Users/Jose Pablo Garcia/Box Sync/A - JP Files/Personal/ITAM/Economía Computacional/Tarea 2/graphs/roc_tree.jpeg")


gg_rocrf <- ggplot(roc_rf, aes(x=specificity, y=sensitivity))+
  geom_abline(slope = -1, intercept = 1, linetype= 'dashed')+
  geom_path()+ 
  labs(title = "Curva Roc",
       subtitle = "Modelo Random Forest",
       x = "Especificidad", y = "Sensibilidad")

gg_rocrf + theme_economist()+
  scale_color_economist()

ggsave(gg_rocrf, filename = "C:/Users/Jose Pablo Garcia/Box Sync/A - JP Files/Personal/ITAM/Economía Computacional/Tarea 2/graphs/roc_rf.jpeg")

```


Los tres modelos tienen un poder de predicción muy pobre, incluso por debajo de un modelo aleatorio. El "menos peor" es el CV LASSO.


### 18. Genera una tabla con el AUC ROC. Cuál es el mejor modelo ? 

```{r 18. Tabla AUC ROC}
auc_lasso <- roc_auc(data = predicciones, truth = obs, "lasso_cvmin")
auc_tree <- roc_auc(predicciones, truth = obs, "pruned_score")
auc_rf <- roc_auc(predicciones, truth = obs, "rf_prob")

auc_lasso
auc_tree
auc_rf

Modelos <- c("CV LASSO", "CV Pruned tree", "Random forest")
AUC <- c(auc_lasso$.estimate, auc_tree$.estimate, auc_rf$.estimate)
AUC_ROC <- cbind(Modelos, AUC)
kable(AUC_ROC,
      digits=3,
      caption = "AUC ROC")

cat("El AUC más grande es el del RF con:", auc_rf$.estimate,"\n",
    "seguido por el pruned_tree con:", auc_tree$.estimate, "\n",
    "y en último lugar el logit lasso cv con:", auc_lasso$.estimate)
```


Todos los modelos tienen un AUC por debajo de 0.5, lo que indica que en general son malos prediciendo.


### 19 (2pts). Escoge un punto de corte para generar predicciones categoricas para el LASSO basado en la Curva ROC. Genera las matrices de confusión para cada modelo. Compáralas. Qué tipo de error es mas pernicioso? 

Queremos que nuestro modelo sea mas sensible que especifico debido a que es mas importante controlar los falsos negativos (error del tipo 2), que los falsos positivos (error del tipo 1). Esto debido a que el costo de clasificar mal a un cliente que va a abandonar el producto, posiblemente sea mayor a implementar esfuerzos de retencion sobre aquellos que el modelo predice abandonaran pero no lo hacen.


Queremos que nuestro modelo sea mas sensible que especifico debido a que es mas importante controlar los falsos negativos (error del tipo 2), que los falsos positivos (error del tipo 1). 

El error tipo 1 es predecir que se va un cliente y que en realidad no se iba a ir. Es decir asignar ineficientemente recursos limitados en donde no se necesitaba.

El error tipo 2 es no predecir que se va un cliente y que si se te vaya. Creemos que el esfuerzo tiene que concentrarse en la sensibilidad, que quienes decimos que se van, encontrarlos realmente. 

$ sensibilidad = \frac{Verdadero \: positivo}{Falsos \: negativos + Verdadero \: positivo}  $

El falso negativo es decir que el cliente no se va, no enfocar esfuerzos y que se termine yendo.


```{r 19. Matriz de confusion}
#MON
#Punto de corte: 0.5
#predic LASSO
predicciones <- predicciones %>%
  mutate(y_lasso_cat = ifelse(Y_lasso < 0.5, 0, 1),
         y_pruned_cat = ifelse(Y_lasso < 0.5, 0, 1))
# Matriz de confusión
 paste("La matriz de confusión para el CV LASSO es:" )
 table(predicted = predicciones$y_lasso_cat, actual = test$churn)  
 
  paste("La matriz de confusión para el CV Pruned tree es:" )
 table(predicted = predicciones$y_pruned_cat, actual = test$churn)  
 
  paste("La matriz de confusión para el Random Forest es:" )
 table(predicted = predicciones$Y_randomfor, actual = test$churn)  
 
 # Función 'accuracy'
accuracy = function(actual, predicted) {
 mean(actual == predicted)*100
}
 paste("El porcentaje de acierto de un CV LASSO es de:", 
       accuracy(predicted = predicciones$y_lasso_cat, actual = test$churn), "%")
 
paste("El porcentaje de acierto de un CV Pruned tree  es de:", 
       accuracy(predicted = predicciones$y_pruned_cat, actual = test$churn), "%")
  
 paste("El porcentaje de acierto de un Random Forest es de :", 
       accuracy(predicted = predicciones$Y_randomfor, actual = test$churn), "%")
```

Se elige como punto de corte 0.5, por lo que todo valor <0.5 se considera como 0, y cualquier valor >=0.5 se considera como 1. 

El Accuracy es mayor para el modelo de Random Forest, con un porcentaje de acierto del 59%.

### 20 (2pts). Finalmente, construye una lift table. Esto es, para 20 grupos del score predecido, genera 1) El promedio de las predicciones, 2) el promedio del churn observado. Existe monotonía? El mejor algoritmo es monotónico? (Tip: usa `ntile` para generar los grupos a partir de las predicciones)


```{r 20. Lift table}
# Modelo LASSO Logit
#  Generamos 20 grupos de clientes de acuerdo a su score de abandono predecido y calculamos el promedio de las probabilidades de abandono por cuantil. 
lift_table_lasso <- abandono%>%select(churn, pred_prob_lasso, abandono_lasso)%>%
  mutate_at(c("churn","pred_prob_lasso","abandono_lasso"), ~as.numeric(.))%>%
  mutate(score = as.integer(ntile(pred_prob_lasso, n=20)),
         churn = case_when(churn==1~0,
                           churn==2~1),
         abandono_lasso = case_when(abandono_lasso==1~0,
                                    abandono_lasso==2~1))%>%
  group_by(score)%>%
  summarise(observado = mean(churn)*100,
            prediccion = mean(abandono_lasso)*100)%>%
  pivot_longer(cols = c(observado,prediccion))
# Grafica
ggplot(lift_table_lasso, aes(x=score, y=value, fill=name))+
  geom_point(shape=21, size=2)+
  geom_path()+
  labs(title = "Modelo LASSO Logit",
       x="Cuantil de Probabilidad de Abandono (20 Grupos)",
       y = "Porcentaje")+
  theme_bw()+
  theme(axis.text = element_text(size=12), text = element_text(size=12), legend.position = "bottom")+
  ggsave(height = 6, width = 6, filename = "Graficas/lift_table_lasso.png")
  
```

```{r}
# Modelo - Pruned Tree 
lift_table_tree <- abandono%>%select(churn, pred_prob_tree, abandono_tree)%>%
  mutate_at(c("churn","pred_prob_tree","abandono_tree"), ~as.numeric(.))%>%
  mutate(score = as.integer(ntile(pred_prob_tree, n=20)),
         churn = case_when(churn==1~0,
                           churn==2~1),
         abandono_tree = case_when(abandono_tree==1~0,
                                    abandono_tree==2~1))%>%
  group_by(score)%>%
  summarise(observado = mean(churn)*100,
            prediccion = mean(abandono_tree)*100)%>%
  pivot_longer(cols = c(observado,prediccion))
# Grafica
ggplot(lift_table_tree, aes(x=score, y=value, fill=name))+
  geom_point(shape=21, size=2)+
  geom_path()+
  labs(title = "Modelo Pruned Tree",
       x="Cuantil de Probabilidad de Abandono (20 Grupos)",
       y = "Porcentaje")+
  theme_bw()+
  theme(axis.text = element_text(size=12), text = element_text(size=12), legend.position = "bottom")+
  ggsave(height = 6, width = 6, filename = "Graficas/lift_table_tree.png")
```


```{r}
#Modelo Random Forest
lift_table_rf <- abandono%>%select(churn, pred_prob_forest, abandono_rf)%>%
  mutate_at(c("churn","pred_prob_forest","abandono_rf"), ~as.numeric(.))%>%
  mutate(score = as.integer(ntile(pred_prob_forest, n=20)),
         churn = case_when(churn==1~0,
                           churn==2~1),
         abandono_rf = case_when(abandono_rf==1~0,
                                    abandono_rf==2~1))%>%
  group_by(score)%>%
  summarise(observado = mean(churn)*100,
            prediccion = mean(abandono_rf)*100)%>%
  pivot_longer(cols = c(observado,prediccion))
# Grafica
ggplot(lift_table_rf, aes(x=score, y=value, fill=name))+
  geom_point(shape=21, size=2)+
  geom_path()+
  labs(title = "Modelo Random Forest",
       x="Cuantil de Probabilidad de Abandono (20 Grupos)",
       y = "Porcentaje")+
  theme_bw()+
  theme(axis.text = element_text(size=12), text = element_text(size=12), legend.position = "bottom")+
  ggsave(height = 6, width = 6, filename = "Graficas/lift_table_rf.png")



```


### 21. Concluye. Que estrategia harías con este modelo? Cómo generarías valor a partir de el?

Se puede pensar en adaptar una estrategia de retencion enfocada hacia los clientes clasificados como propensos a abandonar. Dado que algunas de las variables relevantes fueron el numero de dias que llevaban con el equipo (>360 dias eran propensos a abandonar) se puede pensar en ofrecerles alguna promocion para renovar su equipo de celular. Adicionamente, una de las variables que ayuda a determinar el abandono fue si el cliente experimento una reparacion de celular, por lo que puede pensarse en buscar renovar los equipos dañados. 
